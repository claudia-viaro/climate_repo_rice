# Copyright (c) 2022, salesforce.com, inc and MILA.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root
# or https://opensource.org/licenses/BSD-3-Clause

# Checkpoint saving setting
saving:
  # model parameters (checkpoints)
  save_checkpoints: True # set False to skip model checkpoint saving
  model_params_save_freq: 100 #500 # how often (in iterations) to save the model parameters

  # policy algorithm parameters
  save_policy_info: True # set False to skip policy learning metrics saving


  metrics_log_freq: 100 # how often (in iterations) to log (and print) the metrics
  


# Trainer settings
trainer:
  algorithm: "A2C"  # or "PPO", "SAC"
  num_envs_per_worker: 1 # number of environment replicas per worker
  train_batch_size: 1200 #2100 #2000 #1200 #1800 #2000 #2000 # total batch size used for training per iteration (across all the environments)
  num_episodes: 32000 #40000  #60 #6000 #35000 # 60  300 #3000 #8000 #12000 #50000 #500 #500 #600 #100 #100000 #300 #300 #10000 #100000 # number of episodes to run the training for
  framework: torch # framework setting.
  # Note: RLlib supports TF as well, but our end-to-end pipeline is built for Pytorch only.
  # === Hardware Settings ===
  num_workers: 0 # number of rollout worker actors to create for parallel sampling.
  # Note: Setting the num_workers to 0 will force rollouts to be done in the trainer actor.
  # do not set to -1
  num_cpus_per_worker: 1 # do not set to -1
  num_gpus: 0 # number of GPUs to allocate to the trainer process. This can also be fractional (e.g., 0.3 GPUs).
  num_gpus_per_worker: 0 # required by certain ray algos
# Environment configuration
env:
  negotiation_on: True # flag to indicate whether negotiation is allowed or not
  scenario: "default" # key that maps to either Rice or and alternate Rice class
  num_discrete_action_levels: 10
  action_space_type: "discrete"
  dmg_function: "base"
  carbon_model: "base"
  temperature_calibration: "base"
  pct_reward: False
  clubs_enabled: False
  club_members: [1]
  
  action_window: True
  actions_masked: {
    export_limit: 0, 
    import_bids: 0,
    import_tariffs: 0}
  regions:
    num_agents: 3 #can be either {3,7,20,27}

# Policy network settings
policy:
  multi_model: False
  regions:
    # Common hyperparameters (all algorithms)
    gamma: 0.99
    lr: 0.0005
    model:
      custom_model: torch_linear_discrete
      custom_model_config:
        fc_dims: [256, 256]

    # A2C / PPO specific
    vf_loss_coeff: 0.1
    entropy_coeff_schedule:
      - [0, 0.5]
      - [1000000, 0.1]
      - [5000000, 0.05]
    max_grad_norm: 0.5   # mapped to grad_clip in RLlib
    clip_param: 0.2      # PPO only
    lambda_: 0.95        # PPO only

    # SAC specific
    tau: 0.005
    target_entropy: "auto"
    alpha: 0.2
